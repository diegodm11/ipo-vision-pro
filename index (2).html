<!DOCTYPE html>
<html lang="es">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Apple Vision Pro como dispositivo de interacción | IPO</title>
  <link rel="stylesheet" href="styles.css">
  <meta name="description" content="Trabajo de la asignatura Fundamentos de la Interacción Persona‑Ordenador: análisis de Apple Vision Pro como dispositivo de interacción.">
</head>
<body>
  <header class="site-header">
    <div class="container header-inner">
      <h1>Apple Vision Pro: un nuevo paradigma de interacción</h1>
      <nav class="topnav" aria-label="Secciones principales">
        <a href="#que-es">Qué es</a>
        <a href="#beneficios">Beneficios en IPO</a>
        <a href="#como-funciona">Cómo funciona</a>
        <a href="#evolucion">Evolución</a>
        <a href="#galeria">Galería</a>
        <a href="#recursos">Más información</a>
      </nav>
    </div>
  </header>

  <main class="container">
    <section id="que-es" class="card">
      <h2>¿Qué es y por qué lo he elegido?</h2>
      <p>
        <strong>Apple Vision Pro</strong> es un casco de <em>computación espacial</em> que permite
        interactuar con aplicaciones digitales en el espacio físico.
        Lo he elegido porque es <strong>reciente</strong> (lanzamiento en 2024)
        y porque introduce una combinación natural de <strong>ojos + manos + voz</strong>
        para apuntar, seleccionar, desplazarse y dictar sin mandos físicos.
        Según Apple, basta con <em>mirar</em> un elemento para llevar el foco y
        <em>pellizcar</em> con los dedos para seleccionar, o hacer un <em>flick</em> para desplazarse;
        también se integra con Siri para dictado y comandos de voz.
      </p>
      <p class="fuente">Fuente: Página oficial y guía de gestos de Apple Vision Pro. Ver enlaces en <a href="#recursos">Más información</a>.</p>
    </section>

    <section id="beneficios" class="card">
      <h2>Beneficios que aporta en la Interacción Persona‑Ordenador</h2>
      <ul class="benefits">
        <li><strong>Interacción multimodal y de baja fricción:</strong> coordina mirada (apuntar), gesto (confirmar) y voz (texto/órdenes),
          reduciendo la carga motora y la necesidad de controladores dedicados.</li>
        <li><strong>Contexto espacial:</strong> las ventanas y controles se sitúan en 3D a escala real, lo que favorece la memoria espacial y la organización del trabajo.</li>
        <li><strong>Accesibilidad:</strong> funciones como <em>Dwell/Atención</em> y el dictado ayudan a usuarios con movilidad reducida; la autenticación biométrica <em>Optic ID</em> evita introducir contraseñas.</li>
        <li><strong>Privacidad de la señal:</strong> el sistema procesa localmente mapas de ojos y manos y comparte con las apps sólo <em>intenciones</em> (p. ej., dónde miras), lo que reduce riesgos de datos crudos.</li>
        <li><strong>Transferencia de habilidades:</strong> gestos naturales (mirar, pellizcar) tienen curva de aprendizaje corta frente a atajos o dispositivos abstractos.</li>
      </ul>
    </section>

    <section id="como-funciona" class="card">
      <h2>¿Cómo funciona la interacción?</h2>
      <div class="columns">
        <div>
          <h3>Ojos: apuntar</h3>
          <p>El <strong>seguimiento ocular</strong> selecciona el objetivo (como mover el puntero o “hover”).
             visionOS utiliza la mirada como sistema de <em>targeting</em> primario, con realimentación visual sutil.</p>
        </div>
        <div>
          <h3>Manos: confirmar y manipular</h3>
          <p>Con las manos en reposo (p. ej., sobre el regazo) se detectan <strong>pellizcos</strong> para seleccionar
             y un gesto de <strong>flick</strong> para desplazarse. También hay gestos para ampliar menús o arrastrar.</p>
        </div>
        <div>
          <h3>Voz: texto y comandos</h3>
          <p><strong>Siri y dictado</strong> permiten buscar, introducir texto o abrir apps sin teclado físico.</p>
        </div>
      </div>
      <p class="nota">Apple documenta estos gestos en su soporte oficial y materiales de visión general de visionOS.</p>
    </section>

    <section id="evolucion" class="card">
      <h2>Evolución histórica: ¿qué sustituye y cómo mejora lo anterior?</h2>
      <ol class="timeline">
        <li><span class="fecha">Años 1960‑80</span> — <strong>Teclado y ratón</strong>: precisión y edición textual, pero 2D y periféricos físicos.</li>
        <li><span class="fecha">2007+</span> — <strong>Pantallas táctiles</strong>: gesto directo sobre la interfaz 2D (iPhone/iPad).</li>
        <li><span class="fecha">2010‑2016</span> — <strong>Control gestual sin contacto</strong> (Kinect, <em>Leap Motion</em>): seguimiento de manos/cuerpo; preciso en demos, limitado en uso diario y contexto.</li>
        <li><span class="fecha">2016‑2023</span> — <strong>VR con mandos e <em>inside‑out</em></strong>: controladores hápticos y, progresivamente, <strong>hand‑tracking</strong> (Quest) con mejoras notables.</li>
        <li><span class="fecha">2024</span> — <strong>Convergencia en Vision Pro</strong>: mirada para apuntar + manos para confirmar + voz para texto. Reduce hardware (sin mandos), mejora precisión y comodidad desde posturas naturales.</li>
      </ol>
      <p class="fuente">Fuentes: historia de Leap Motion/Ultraleap; mejoras de hand‑tracking en Meta Quest; literatura de seguimiento ocular en HCI.</p>
    </section>

    <section id="galeria" class="card">
      <h2>Galería</h2>
      <figure>
        <img src="https://www.apple.com/v/apple-vision-pro/c/images/overview/hero/hero_endframe__dz973m6qw1ua_xlarge.jpg" alt="Apple Vision Pro mostrando ventanas en un entorno real">
        <figcaption>Interfaz espacial en Vision Pro (imagen de Apple).</figcaption>
      </figure>
      <figure>
        <img src="https://www.apple.com/newsroom/images/product/vision/Apple-WWDC23-visionOS-hero_big.jpg.large.jpg" alt="Gestos de manos para seleccionar y desplazarse en visionOS">
        <figcaption>Gestos de manos y mirada como entrada primaria.</figcaption>
      </figure>
    </section>

    <section id="recursos" class="card">
      <h2>Más información y referencias</h2>
      <ul class="refs">
        <li>Apple — <a href="https://www.apple.com/apple-vision-pro/" target="_blank" rel="noopener">Apple Vision Pro (oficial)</a></li>
        <li>Apple Soporte — <a href="https://support.apple.com/en-mide/117741" target="_blank" rel="noopener">Usar gestos con Apple Vision Pro</a></li>
        <li>Apple Privacidad — <a href="https://www.apple.com/privacy/docs/Apple_Vision_Pro_Privacy_Overview.pdf" target="_blank" rel="noopener">Visión general de privacidad en Vision Pro (PDF)</a></li>
        <li>UploadVR — <a href="https://www.uploadvr.com/apple-vision-pro-gesture-controls/" target="_blank" rel="noopener">Cómo se controla Vision Pro con ojos y manos</a></li>
        <li>Wikipedia — <a href="https://es.wikipedia.org/wiki/Leap_Motion" target="_blank" rel="noopener">Leap Motion (historia y contexto)</a></li>
        <li>The Verge — <a href="https://www.theverge.com/2024/12/9/24317237/meta-quest-v72-update-pc-pairing-hand-tracking-keyboard-look" target="_blank" rel="noopener">Actualización v72 con mejoras de hand‑tracking (Quest)</a></li>
      </ul>
    </section>

    <footer class="site-footer">
      <p>Trabajo para <em>Fundamentos de la Interacción Persona‑Ordenador</em> — Fecha de entrega: 11 de noviembre de 2025.</p>
      <p>Autor: Diego Dominguez Martinez <a href="#" aria-disabled="true"></a></p>
    </footer>
  </main>
</body>
</html>
